{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7facec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from diskcache import Cache\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35f82d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_path = Path(os.getcwd() + \"/relations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ffe2dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_relations(directory_path: str):\n",
    "\n",
    "    rows = []\n",
    "    num_to_rel = dict()\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        full_path = os.path.join(directory_path, filename)\n",
    "        temp_data = re.split(\"[-.]\",filename)\n",
    "        relation_num = int(temp_data[0])\n",
    "        relation_name = temp_data[1]\n",
    "        num_to_rel[relation_num] = relation_name\n",
    "        if os.path.isfile(full_path):\n",
    "            \n",
    "            with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = f.read()\n",
    "                data = re.split(\"\\n\",data)\n",
    "                \n",
    "                for l in data:\n",
    "                     rows.append({\"content\": l, \"sem_type\": relation_num})\n",
    "\n",
    "           \n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    print(df.head())\n",
    "    \n",
    "\n",
    "    return df, num_to_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77b86b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraire_relation(phrase):\n",
    "    # on retire Le, La, L’, Les, etc...\n",
    "    phrase = phrase.strip()\n",
    "    phrase = re.sub(r\"^(l['’]|le|la|les|.)\\s+\", \"\", phrase, flags=re.IGNORECASE)\n",
    "    \n",
    "    # regex qui divise la phrase en 3 : le mot A, le connecteur et le mot B :\n",
    "    pattern = r\"^([\\w\\-éèêàùûôîç]+)\\s+(d['’]|du|de la|de l’|de l'|de|des)\\s+(.+)$\"\n",
    "   # print(phrase)\n",
    "    m = re.match(pattern, phrase, flags=re.IGNORECASE)\n",
    "    #print(m)\n",
    "    if not m:\n",
    "        return None\n",
    "   \n",
    "    A = m.group(1)\n",
    "    connecteur = m.group(2)\n",
    "    B = m.group(3).strip()\n",
    "    \n",
    "    return [A, connecteur, B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "824438e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     content  sem_type\n",
      "0    Le travail de l’ouvrier         9\n",
      "1   Le projet de l’ingénieur         9\n",
      "2  Le discours du professeur         9\n",
      "3     Le roman de l’écrivain         9\n",
      "4    Le tableau de l’artiste         9\n"
     ]
    }
   ],
   "source": [
    "# creation du dataset\n",
    "df, relations_dict = load_relations(relations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a1d48fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{9: ' Agent', 13: ' Conséquence', 5: ' Matière', 3: ' Holonymie', 15: ' Caractéristique', 11: ' Sujet', 16: ' Topic', 4: ' Quantification', 8: ' Lieu', 10: ' Auteur', 14: ' Possession', 2: ' Lien social', 1: ' Origine'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(relations_dict)\n",
    "temp_df = df['content'].apply(lambda x: extraire_relation(x))\n",
    "temp_df = temp_df.dropna()\n",
    "temp_df = temp_df.apply(lambda x: [x[0].lower(),x[1].lower(),re.sub(R\"(^l’|\\.$)\",'',x[2]).lower()])\n",
    "\n",
    "df['content'] = temp_df\n",
    "df= df.dropna()\n",
    "train_ds = []\n",
    "for index, row in df.iterrows():\n",
    "    words = row['content']\n",
    "    train_ds.append((words[0],words[2],row['sem_type']))\n",
    "\n",
    "#print(train_ds[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e638b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonctions pour la similarite cosinus\n",
    "def norm(v):\n",
    "    res = 0\n",
    "    for x in v:\n",
    "        res += (x *x)\n",
    "    return res**(1/2)\n",
    "\n",
    "def dot(v1,v2):\n",
    "    if(v1.shape != v2.shape):\n",
    "        raise ArithmeticError\n",
    "    res = 0\n",
    "    for x,y in zip(v1,v2):\n",
    "        res += x*y\n",
    "    return res\n",
    "        \n",
    "def cosine_similarity(v1,v2):\n",
    "    return dot(v1,v2)/(norm(v1) * norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40e694d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classe qui cree la signature d'un terme et s'occupe de les cacher pr limiter le nombre de req\n",
    "class SignatureLoader:\n",
    "    def __init__(self,cache_dir=\"./jdm_cache\"):\n",
    "         self.cache = Cache(cache_dir)\n",
    "            \n",
    "        \n",
    "    def get_signature(self, term):\n",
    "        \n",
    "        if term in self.cache:\n",
    "            return self.cache[term]     \n",
    "        sig = set()\n",
    "        # peut-etre prends les relation /to/{term} ?\n",
    "        res = requests.get(f\"https://jdm-api.demo.lirmm.fr/v0/relations/from/{term}\")\n",
    "        #print(res)\n",
    "        if(res.status_code != 200):\n",
    "            return sig\n",
    "        try:\n",
    "            res = json.loads(res.text)\n",
    "        except:\n",
    "            #print(\"Error decoding response\")\n",
    "            return sig\n",
    "        \n",
    "        try:\n",
    "            for r in res[\"relations\"]:\n",
    "\n",
    "                if(r[\"w\"] <= 0): continue\n",
    "\n",
    "                if(r[\"type\"] == 36):\n",
    "                    sig.add(r[\"node2\"])\n",
    "                elif(r[\"type\"] == 6):\n",
    "                    sig.add(r[\"node2\"])\n",
    "\n",
    "                sig.add(r[\"type\"])\n",
    "\n",
    "                self.cache.set(term, sig, expire=None)\n",
    "        except:\n",
    "            return sig\n",
    "\n",
    "        return sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e5f0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une regle est composee de 2 mots A et B, d'une relation et d'un \"poids\" (nombre de fusions)\n",
    "class Rule:\n",
    "    def __init__(self, sigA, sigB, relation, weight=1):\n",
    "        self.sigA = sigA\n",
    "        self.sigB = sigB\n",
    "        self.relation = relation\n",
    "        self.weight = weight\n",
    "\n",
    "    def fuse(self, other):\n",
    "        return Rule(\n",
    "            set.union(self.sigA , other.sigA),\n",
    "            set.union(self.sigB , other.sigB),\n",
    "            self.relation,\n",
    "            self.weight + other.weight\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeaa3597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# les signautres sont des vecteurs sparse avec des 0 et des 1\n",
    "def signature_to_vector(sig, vocab):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    for w in sig:\n",
    "        vec[vocab[w]] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9c00bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_rules(examples, backend, threshold=0.5):\n",
    "    # threshold definit la similarite minimale pour la fusion de 2 regles\n",
    "    rules = []\n",
    "\n",
    "    for A, B, rel in examples:\n",
    "        sigA = backend.get_signature(A)\n",
    "        sigB = backend.get_signature(B)\n",
    "\n",
    "        new_rule = Rule(sigA, sigB, rel)\n",
    "\n",
    "        fused = False\n",
    "        for r in rules:\n",
    "            if r.relation != rel:\n",
    "                continue\n",
    "\n",
    "            vocab = {w: i for i, w in enumerate(set.union(r.sigA , new_rule.sigA))}\n",
    "            v1 = signature_to_vector(r.sigA, vocab)\n",
    "            v2 = signature_to_vector(new_rule.sigA, vocab)\n",
    "            if(len(v1) == 0): continue\n",
    "            simA = cosine_similarity(v1,v2)\n",
    "           # print(simA)\n",
    "\n",
    "            vocab = {w: i for i, w in enumerate(set.union(r.sigB , new_rule.sigB))}\n",
    "            v1 = signature_to_vector(r.sigB, vocab)\n",
    "            if(len(v1) == 0): continue\n",
    "            v2 = signature_to_vector(new_rule.sigB, vocab)\n",
    "\n",
    "            simB = cosine_similarity(v1,v2)\n",
    "\n",
    "            if (simA + simB) / 2 >= threshold:\n",
    "                merged = r.fuse(new_rule)\n",
    "                rules.remove(r)\n",
    "                rules.append(merged)\n",
    "                fused = True\n",
    "                break\n",
    "\n",
    "        if not fused:\n",
    "            rules.append(new_rule)\n",
    "\n",
    "    return rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29318a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(A, B, backend, rules):\n",
    "    best_score = -1\n",
    "    best_rel = None\n",
    "    # 1 - on calcul la signature des 2 termes\n",
    "    sigA = backend.get_signature(A)\n",
    "    sigB = backend.get_signature(B)\n",
    "\n",
    "# 2 - on cherche la meilleure similarite cosinus entre toutes les regles de notre corpus\n",
    "    for r in rules:\n",
    "        # vocab = ensemble des donnees capturees dans les 2 signatures\n",
    "        vocabA = {w: i for i, w in enumerate(set.union(sigA, r.sigA))}\n",
    "        vecA = signature_to_vector(sigA, vocabA)\n",
    "        vecRA = signature_to_vector(r.sigA, vocabA)\n",
    "        simA = cosine_similarity(vecA,vecRA)\n",
    "    \n",
    "\n",
    "\n",
    "        vocabB = {w: i for i, w in enumerate(set.union(sigB, r.sigB))}\n",
    "        vecB = signature_to_vector(sigB, vocabB)\n",
    "        vecRB = signature_to_vector(r.sigB, vocabB)\n",
    "        simB = cosine_similarity(vecB,vecRB)\n",
    "        \n",
    "        \n",
    "\n",
    "        score = (simA + simB) / 2\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_rel = r.relation\n",
    "\n",
    "    return best_rel, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f765203",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = SignatureLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ec8cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Entrainement\n",
    "\n",
    "#training = train_ds[:100]\n",
    "\n",
    "training = train_ds\n",
    "rules = learn_rules(training, sl,threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5446d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT SAUVEGARDER LES REGLES APPRISES\n",
    "def save_rules(rules, filename=\"rules.pkl\"):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(rules, f)\n",
    "\n",
    "def load_rules(filename=\"rules.pkl\"):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780a2f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_rules(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb7a3fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1052f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7269/79870060.py:17: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return dot(v1,v2)/(norm(v1) * norm(v2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Agent 0.7591390799850493\n"
     ]
    }
   ],
   "source": [
    "# Inferences\n",
    "A, conn, B = extraire_relation(\"\")\n",
    "rel, score = classify(A, B, sl, test)\n",
    "\n",
    "print(relations_dict[rel], score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb767f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine-Learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
